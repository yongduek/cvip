{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Pose: Perspective n Point Algorithm\n",
    "\n",
    "**https://en.wikipedia.org/wiki/Perspective-n-Point**\n",
    "\n",
    "- Perspective-n-Point [1] is the problem of estimating the pose of a calibrated camera given a set of n 3D points in the world and their corresponding 2D projections in the image. \n",
    "- The camera pose consists of 6 degrees-of-freedom (DOF) which are made up of the rotation (roll, pitch, and yaw) and 3D translation of the camera with respect to the world. \n",
    "- This problem originates from camera calibration and has many applications in computer vision and other areas, including 3D pose estimation, robotics and augmented reality.[2] \n",
    "- A commonly used solution to the problem exists for n = 3 called P3P, and many solutions are available for the general case of n ≥ 3. \n",
    "- A solution for n = 2 exists if feature orientations are available at the two points.[3] \n",
    "- Implementations of these solutions are also available in open source software: OpenCV\n",
    "\n",
    "[1]  Fischler, M. A.; Bolles, R. C. (1981). \"Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography\". Communications of the ACM. 24 (6): 381–395. doi:10.1145/358669.358692."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projective Camera with Direct Linear Transformation Algorithm\n",
    "\n",
    "- [HZ book Chapter 7, P.179, p.91]\n",
    "\n",
    "<div>\n",
    "    <img src=\"camcal/hz-eq.7.1.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "- $P$ has 12 entries, 11 degrees of freedom.\n",
    "- Apply DLT Algorithm (below is for $H$ which is a $3\\times3$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"camcal/hz_dlt_algo.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to do data **normalization** transformation before any numerical computation. Then post-process the result so that it can be used with the original data (or orginal coordinate system)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"camcal/hz_dlt_algo_normalization.png\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton Algorithm\n",
    "- https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm\n",
    "- The Gauss–Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function.\n",
    "- Unlike Newton's method, the Gauss–Newton algorithm can only be used to minimize a **sum of squared function values**, but it has the advantage that second derivatives, which can be challenging to compute, are not required.[1]\n",
    "- Uses approximated **Hessian**\n",
    "\n",
    "Sum of squares function:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/13eb424e8c2129defe1269780a294d1b528c2021)\n",
    "\n",
    "The 2nd order approximation\n",
    "\n",
    "$$\n",
    "    r (\\beta + \\delta) = r(\\beta) + J^T \\delta\n",
    "$$\n",
    "\n",
    "Update Equation:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/88e33e397c61b5818d2d04168447490dc9b630ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt Algorithm\n",
    "\n",
    "$$\n",
    "    \\beta^{(s+1)} = \\beta^s + (J^T J + \\lambda\\mathrm{diag}(J^T J))^{-1} J^T r\n",
    "$$\n",
    "\n",
    "- see camera_calib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "- https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n",
    "- Newton's method is applied to the derivative $f′$ of a twice-differentiable function $f$ to find the roots of the derivative (solutions to $f′(x) = 0$), also known as the stationary points of $f$. These solutions may be minima, maxima, or saddle points.[1]\n",
    "- Uses **Hessian** to find a stationary point of $f$\n",
    "\n",
    "Update:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/d8b266ce1ac150b43acb59eca9abf45fc08a33db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent algorithm\n",
    "- https://en.wikipedia.org/wiki/Gradient_descent\n",
    "- Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function.\n",
    "- Also known as steepest descent.\n",
    "- Move in the direction of the negative gradient of $F$.\n",
    "\n",
    "Update:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0154a26cc6ac60465f8eb3d00d2f2dfa6899da2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient Algorithm\n",
    "- https://en.wikipedia.org/wiki/Conjugate_gradient_method\n",
    "- In mathematics, the conjugate gradient method is an algorithm for the numerical solution of particular systems of linear equations, namely those whose matrix is symmetric and positive-definite.\n",
    "- Normally used to solve a system of linear equations\n",
    "\n",
    "$$\n",
    "    A x = b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "-  It can be regarded as a stochastic approximation of gradient descent optimization.\n",
    "\n",
    "Objective Function:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/7f38e0bdfea090cfd651222e7db9806dce6164cd)\n",
    "\n",
    "where $w$ is the parameter vector of interest\n",
    "\n",
    "- Each summand function $Q_{i}$ is typically associated with the $i$-th observation in the data set.\n",
    "\n",
    "A standard gradient descent method performs the following:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/4895d44c0572fb2988f2f335c28cc055a7f75fa0)\n",
    "\n",
    "**Iteration**\n",
    "\n",
    "The true gradient $\\nabla Q$ is approximated by a gradient at a single example:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4)\n",
    "\n",
    "- Notice that $1/n$ is not included here. One sample gradient scaled by $n$ is equivalent to: \n",
    "\n",
    "$$\n",
    "    \\nabla Q_i := \\frac{1}{n}\\sum_{k=1}^n \\nabla Q_i\n",
    "$$\n",
    "\n",
    "1. Choose initial vector $w$ and learning rate $\\eta$\n",
    "2. Repeat until convergence:\n",
    "    1. randomly shuffle\n",
    "    2.for each data, $i = 1, 2, ..., n$:\n",
    "    \n",
    "    $$\n",
    "        w := w - \\eta \\nabla Q_i(w)\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Line fitting problem: $\\hat{y} = w_1 + w_2 x$ given a data set $(x_i, y_i)$.\n",
    "\n",
    "Objective function:\n",
    "\n",
    "$$\n",
    "    Q(w) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}(x_i))^2\n",
    "$$\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/87481666e82c57e7924564d5208e22946ddfb4f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "$$\n",
    "    w := w + \\eta \\nabla Q_i(w) + \\alpha \\Delta w\n",
    "$$\n",
    "\n",
    "where\n",
    "$ \\Delta w$ is the update at the previous step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent\n",
    "- https://en.wikipedia.org/wiki/Coordinate_descent\n",
    "- Coordinate descent is an optimization algorithm that successively minimizes along coordinate directions to find the minimum of a function.\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/0c4701fba427fc0fc77d0313ffb5c98edb553251)\n",
    "\n",
    "Limitations:\n",
    "\n",
    "1. For non-smooth multivariable functions, this algorithm may get stuck at a non-stationary point.\n",
    "2. Difficulty in parallelism\n",
    "\n",
    "Applications:\n",
    "\n",
    "1. tomography where it has been found to have rapid convergence\n",
    "2. machine learning; CD has been shown competitive to other methods when applied to large scale problems; training linear SVM and non-negative matrix factorization.\n",
    "3. Any optimization problems where computing full gradients is infeasible.\n",
    "    - Nesterov, Yurii (2012). \"Efficiency of coordinate descent methods on huge-scale optimization problems\" (PDF). SIAM J. Optim. 22 (2): 341–362. CiteSeerX 10.1.1.332.3336. doi:10.1137/100802001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
